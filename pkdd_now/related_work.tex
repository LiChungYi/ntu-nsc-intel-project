\section{Related Work}\label{sec:rw}


\begin{table}[h]
\vspace{-0.7cm}
\caption{Sensor Data Imputation Methods}
\label{tbl:methods}
\centering
{\small
\begin{tabular}{|l|l|l|} \hline
   &{\bf Hot-Deck Imputation}&{\bf Prediction Models}\\ \hline
{\bf Temporal} & Last-seen~\cite{Granger:lastseen}, Mean & Linear Interpolation\\ \hline
{\bf Spatial} & WARM~\cite{le2005estimating}, FARM~\cite{Gruenwald:FARM} & DEPM~\cite{li2008data}, MI~\cite{yuan2000multiple}\\\hline
{\bf Spatio-} & STI~\cite{Jian-Zhong:STI} & DESM~\cite{li2008data}, ImM~\cite{Lim:robust}, AKE~\cite{pan2010k},\\
{\bf Temporal} && BGP~\cite{osborne2012real}, EOF~\cite{beckers2003eof, kondrashov2006spatio}\\
 \hline \end{tabular}
}
\vspace{-0.1in}
\end{table}

Imputation techniques applied to sensor data can be divided into three categories by the information utilized:
temporal methods, spatial methods, 
% (i.e., estimation using the observations from the target sensor at nearby time steps), 
% (i.e., estimation using neighboring sensor node observations), 
and spatio-temporal methods. They can be further categorized as hot-deck imputation and prediction models~\cite{Garcia:KNNreview},
as shown in Table~\ref{tbl:methods}.
%Hot-deck imputation methods directly fill in the missing values using either neighbor values or historical records from itself 
%such as the \textit{last-seen} method, while the prediction models exploit a function (involving metrics other than just sensor 
%values) to estimate the missing values. 

%The feasibility of estimating missing sensor observations based on historical data is grounded by the known temporal correlation in WSN data~\cite{akyildiz2004exploiting}.
%Moreover, where there is a potential for global communication issues to affect the availability for sensor node observations \emph{en masse} during a given duration of time, utilizing spatial correlations as a basis for estimation may be not be possible.

\subsection{Temporal Methods}
{\em Temporal methods} leverage the temporal correlation among
readings by the same sensor node; salient methods include observed
data mean~\cite{madden2005tinydb}, last
seen~\cite{Granger:lastseen}, and linear interpolation. %:
%the estimated value $\hat{y_{it}}$ for sensor $i$ at time $t$ is
%$\hat{y_{it}} = y_{iu} + \frac{y_{iv}-y_{iu}}{T_v-T_u}(t-T_u)$,
%where $y_{iu}$ ($y_{iv}$) at time $T_u$ ($T_v$) is the first 
%observation for sensor $i$ prior to (after, respectively) time $t$.
%
These methods
suffer, however, when there are long temporal gaps in observations for a given
sensor; such gaps can be frequent in WSNs due to power depletion in
energy-harvesting sensors, long-lived communication ailments, etc.  
As a result, the usefulness of temporal methods drops rapidly as the number of consecutively missing readings
becomes large.

% (i.e., as can happen when intermittent communications starvation occurs in large WSNs).  


\subsection{Spatial Methods}\label{subsec:sm}
{\em Spatial methods} leverage the spatial correlation among readings
by nearby sensor nodes; salient methods include association rule
mining (e.g., WARM~\cite{le2005estimating} and
FARM~\cite{Gruenwald:FARM}) 
%\cite{jiang2007estimating} 
and weighted functions of nearby sensors (e.g., DEPM~\cite{li2008data}
%K-NN\cite{pan2010k}, 
and MI~\cite{yuan2000multiple}).
%Researchers also propose predicting missing values based on data mining techniques. 
Window association rule mining (WARM)~\cite{le2005estimating} and freshness association rule mining (FARM)~\cite{Gruenwald:FARM} study the estimation of missing data based on the association rules among spatially-correlated neighbors. 
Such methods enjoy the advantage of being able to handle categorical sensor data, but the 
estimation quality is limited in continuous sensor data by the association rules' requirement to 
first quantize the data.
%(basically, the continuous data needs to be quantized into suitable
%categories for association matches).
%Moreover, the support, confidence of the mined rules need to be predefined by the users which can cause difficulties for building a satisfying model by users without profound knowledge about the environment of deployment. 
%\subsubsection{K-Nearest Neighbours Imputation} 
%  The two major drawbacks of KNN-based model that predicts only from the spatial correlation are: first of all,  they cannot capture temporal correlation, and the second reason is that most of these methods rely on the knowledge of distance between each sensor, which might not be available or accurate in the real world.
The data estimation using physical model (DEPM)~\cite{li2008data} method employs the basic laws of physics to 
design its prediction function. %as,  
%%\begin{equation}
%$I_K =\sum_{j=1}^M\frac{P_j}{4\pi d^2(I_j,s_k)}$
%%\label{DEPM}
%%\end{equation} 
%where $I_k$ is the intensity of the target sensor value, $j$ represents the neighbor sensors, and P stands for the power radiated from one source to another. 
However, such models are applicable only to limited types of signals and environments, and generally require 
an accurate three-dimensional distance among sensors.
The multiple imputation (MI) method~\cite{yuan2000multiple} imputes the missing data by replacing each missing value with a set of plausible values.
%instead of filling in a single value. 
Methods used in MI for plausible values include multiple linear regression, propensity score method, and 
Markov chain Monte Carlo method.
% The missing data are filled in $M$ times ($M=3-10$~\cite{Little:hotdeck}) 
%to generate $M$ complete datasets, and the results from $M$ generated datasets are averaged to obtain a single prediction~\cite{yuan2000multiple}. 
This model suffers from high computation cost because multiple models have to be learned. 
%Moreover, a poor model might unduly 
%contribute to an inferior averaged result. 
%However, this method is criticized on various grounds by WSNs imputation researchers~\cite{jiang2007estimating,Gruenwald:FARM}. 
%First, in sensor data we do not know how many rounds of information to use in order to get the associated information.
%In addition, it is difficult to draw a pool of similar complete cases for a certain round of a certain sensor. 
%Last, it consumes unnecessary time since the sensor data may or may not related to all of the available information.

Generally speaking, there are two ways to obtain the spatial
correlation: from the spatial coordinates or from the data itself.
However, (only) the former requires accurate spatial coordinates, and
more importantly, it suffers when ailments arise that affect entire
spatial regions (such as large, temporary obstacles to sensing and/or
communication).  It also fails to account for barriers or other
sources of sharp environmental gradients.  For example, two nearby
sensors, one near a stove and one beside a window, can produce very
different values if the stove is in use and the outside temperature is
low.  In the end, using spatial coordinates can often lead to worse
imputation results as non-existent or time-varying correlations are
imposed between nearby sensors.

%For example, in Figure~\ref{house_floorplan}, we find
%sensors $1$ \& $2$ deployed in the kitchen, next to the stove and
%outside window, respectively. While these sensors are nearby, 
%there may be a large temperature difference between the two if
%the stove is in use and the outside temperature is cold.
%Similarly, sensors $2$ \& $5$
%may be quite uncorrelated despite their relative proximity due to the
%wall between them and the presence of kitchen or laundry appliance
%use.  On the other hand, relatively distant sensors $3$ \& $4$
%may be quite correlated as they are within the same room and both
%near an outside wall. The calculation of
%inter-node signal strength or line of sight distance between nodes
%helps to mitigate these issues, though it is not
%a complete remedy.  In the end, using spatial
%coordinates can often lead to worse imputation results.
% as non-existent or time-varying correlations are imposed between nearby sensors.

%Certain methods consider not strictly the distance between sensors,
%but instead establish a ``neighborhood of influence'' whose size
%becomes a tuning parameter, which adds to the complexity of such methods.

%\begin{figure}[t]
%\centering
%\includegraphics[scale=0.3]{house_floorplan.png}
%\caption{Example home floor plan showing five deployed temperature sensors} \label{house_floorplan}
%\label{fig:example_home_floorplan}
%\vspace{-0.1in}
%\end{figure}

\subsection{Spatio-Temporal Methods}   \label{sec:rw_spatial_temporal}
{\em Spatio-temporal methods} consider both the
temporal and spatial correlation; salient methods include
STI~\cite{Jian-Zhong:STI}, DESM~\cite{li2008data}, ImM~\cite{Lim:robust},
AKE~\cite{pan2010k}, BGP~\cite{osborne2012real}, and EOF~\cite{beckers2003eof, kondrashov2006spatio}.
%and Imputation Method~\cite{Lim:robust}.
%These
%have the potential advantage of using both types of correlation in
%imputation, but can suffer from the spatial correlation issues
%discussed above.

The spatial and temporal imputation (STI) method~\cite{li2008spatial}
favors spatial information over temporal information.  For each
missing reading, STI first checks if any nodes are within the sensing
neighborhood (i.e., within a threshold distance), and utilizes the average of these neighbors to impute the
missing value. If no sensors are within the neighborhood, the last
seen value of the missing sensor is used for imputation.
In data estimation using statistical model (DESM)~\cite{li2008data}, a
missing reading is predicted using the linear combination of the
previous reading of the sensor and the current reading of the
neighboring sensor, weighted by the Pearson correlation between the
two sensors.  The imputation method proposed in \cite{Lim:robust}~(ImM) learns to combine two temporal predictors 
(the last-seen predictor and an autocorrelation-based temporal linear predictor) and one spatial 
linear predictor.

%It has the advantage of simple and fast, but suffers when last seen value is far away from the current time stamp, or when most of the nodes in the sensing region has high missing rate.
%Furthermore, the usage of circular region is problematic as most of the landforms might not be uniformly distributed. Finally, determining the range of the radius is tricky, as a larger radius can cause noise while a smaller one cannot highlight the effectiveness of this model(i.e., the results will be similar to linear interpolation).
%The radius of the region is sometimes too large for sensors whose correlations with the target sensor are low in the sensing range.  

%the appropriate weighting of temporal and spatial features by considering two temporal predictors, 

%Given two sensors data streams, $\{X_{i1},X_{i2},\dots,X_{im}\}$ and \\ $\{X_{j3},X_{j2},\dots,X_{jm}\}$, and assuming
%the value of $X_{i(m+1)}$ is missing,
%the prediction on sensor $i$ by its nearest sensor $j$ is:
%\begin{center}
%$\hat{X}_{i(m+1)} = (1-\alpha)X_{im} + (\alpha)\hat{z}$
%\end{center}
%where $\alpha$ is the Pearson correlation coefficient between the sensor $i$ and $j$, and $\hat{z}$ is the estimation of sensor $i$ 
%based on the observation from sensor $j$---it measures the influence of the data sensed by node $i$ on the data of 
%the nearest node $j$:
%\begin{center}
%$\hat{z} = X_{im}(1+\frac{X_{j(m+1)}-X_{j(m)}}{X_{j(m)}})$
%\end{center}
%The above equation assumes that the sensor $X_i$ and $X_j$ have the similar data fluctuation trend. 
%$\alpha$ is expected to have more impact on the prediction if $X_i$ is more correlated with $X_j$. 
%\begin{equation}
%\label{z-hat}
%\end{equation}
%In DESM, because $\alpha$ is usually high around the target sensor. Therefore, the temporal feature multiplied by $1-\alpha$ holds a very small portion of contribution to the prediction process. It often contradicts the nature that the temporal correlation itself is usually higher than any spatial correlation with other sensors. The performance may becomes bad if the nearest sensor is not the most correlated one, i.e., blocked by a wall.
% \subsubsection{Applying K-nearest neighbour Estimation}

%K-nearest neighbors algorithm (KNN) is an intuitive yet effective spatial-correlation-based imputation method. 
%It directly uses the weighted average of the neighbor signals to impute the missing data, and has been adopted to successfully 
%estimate the missing values of DNA micro-arrays~\cite{Troyanskaya:DNAKNN}.  
%While in WSNs, the sensor data of different nodes is more likely to have some functional relations other than just using sensor values.

The Applying K-nearest neighbor Estimation (AKE) method~\cite{pan2010k}
adopts linear regression models to describe the spatial relationship among
each pair of sensor nodes. The prediction of a target node is the
weighted combination of the regression models from its K-nearest neighbors.
If no enough spatial information, AKE incorporates temporal information.
%there is not enough spatial information to be exploited, AKE will use
%temporal information to help.

%\begin{center}
%$y_{it} = \alpha + \beta\cdot y_{jt} + \mu_{jt} $,
%\end{center}
%where $y_{it}$ is the observation of sensor $i$ at time $t$, $y_{jt}$ is the data of sensor $j$ at the same time, 
%and $\mu_{jt}$ is the random error at time t.   
%When the data of sensor $i$ at time $t$ is missing, the estimated value of sensor $i$ by sensor $j$ at the same time is:
%\begin{center}
%$\hat{y}_{it}^{(j)} =\hat{\alpha} +\hat{\beta}\cdot y_{jt}$
%\label{ake_j}
%\end{center}
%The $\hat{\alpha}$ and $\hat{\beta}$ are the estimated values of $\alpha$ and $\beta$, based on the sample data according to the least squares principle.
%They claim that using a linear combination of the estimation from neighbour sensors can lower the random error caused by only using a single sensor. 
%Therefore, given the $m$ nearest neighbor sensors who have values at time $t$[$y_{1t}, y_{2t},\dots, y_{mt}$], the missing value is predicted by :   
% \begin{center}
%$ \hat{y}_{it} =\sum_{j=1}^m w_j \cdot \hat{y}_{it}^{(j)}$
 %\label{ake_impute}
% \end{center}
% First they build a matrix 
%\[ \left( \begin{array}{cccc}
%
%\end{array}
%\right)
%\]
%where $w_j$ is the weight of sensor $j$ used to predict sensor $i$, and is larger for more highly correlated pairs.
%They adopt the r-square statistics to rank their correlations. The value of $w_j$ is the weighted average of the corresponding 
%r-square values for the $m$ sensors. 

The Bayesian Gaussian Process (BGP) method~\cite{osborne2012real} is a
recently proposed random process method that assumes the current
readings are Gaussian distributed given the past data.  The parameter,
mean and covariance are changeable from time to time based on domain
knowledge on the mean and covariance function.  In contrast, we focus
in this paper on sensor data imputation that does not rely on domain
knowledge.

The above methods suffer from making unverified assumptions about the
data. For example, some models favor spatial correlation over temporal
correlation (or vice versa), while some assume that sensors of the
same distance should have a similar correlation.
%regardless of their
%orientation or other factors. 
Such assumptions may or may not hold
for various datasets, and hence imposing them a priori can lead to
inaccurate results.  The models we propose, however, try to rely less
on such a priori knowledge, and learn the latent correlation directly
from the data.

Finally, conceptually closer to our solution are the Singular Value Decomposition (SVD)-based methods.  
%The Singular Value Decomposition (SVD)-based methods exploit the SVD commonly used in linear algebra to impute missing values.
Conventional SVD has a significant
limitation as it can only be applied to a complete matrix.  Therefore,
one needs to somehow first fill in the missing values before
conducting such decomposition.  The initial assignment of those
missing values, unfortunately, can significantly affect the prediction
accuracy~\cite{koren2009matrix}.  Furthermore, SVD is computationally
expensive in general.  
%SVD is widely used as a dimension reduction technique.
%It decomposes a fully observed matrix~$\mathbf{R}$ into one diagonal matrix~$\mathbf{D}$ and two unitary matrices~$\mathbf{U}$ and~$\mathbf{V}$ such that
%$\mathbf{R} = \mathbf{U}\mathbf{D}\mathbf{V}^T.$
%The largest $K$ singular values and vectors, $\mathbf{U}_K \mathbf{D}_K \mathbf{V}_K^T$, form the best $K$-rank approximation of $\mathbf{R}$ under the Frobenius Norm.
A salient example of SVD-based approaches for imputation is the Empirical Orthogonal Functions
(EOF) model, which has been applied to oceanographic applications to
solve the problem of missing or unreliable satellite
data~\cite{beckers2003eof}.  EOF first fills in the missing values
(e.g., using all zeros or the mean values) and then performs SVD 
to decompose the matrix. The top-$K$ singular vectors in the SVD are used 
to reconstruct the matrix and update the estimation of missing values, 
where $K$ is determined through cross validation over the data.
An improved version of EOF~\cite{kondrashov2006spatio} also 
considers the temporal ordering:
The readings from one sensor node are copied $M$ times and form $M$ lag-shifted time series.
The modification boosts the accuracy of EOF, but significantly slows down the method
due to its $M$-fold data size increase.
Moreover, our study shows that the improved EOF is still less accurate than our model.

% The other modification:
% Singular vectors are taken out one at a time => make the determination of $K$ easier.


%In the field of Collaborative Filtering, researchers have already pointed out
%several drawbacks in the SVD-based methods. % when applying to impute missing data. 
%First, given large data, it
%is computational demanding as we need to do the decomposition
%in every iteration. Second, it requires to fill in all missing values
%before the decomposition can be performed, and imperfect initial
%imputation can significantly hurt the final imputation results, in
%particular when the missing data rate is
%high~\cite{koren2009matrix}.

%Hybrid methods of temporal and spatial approaches are less common in the literature.
%For example, the average of the temporal approach of linear interpolation and the spatial approach of multivariate regression has been reported[8].
%Strictly speaking, this approach can be thought of as an ensemble approach between the two methods rather than a fully-integrated approach which considers both temporal and spatial aspects of WSN data.

%\subsection{Research Gap Identification (why are current approaches inadequate?)}
%Accurate imputation of missing sensor network observations is crucial to allow for effective subsequent analysis.
%While there are many existing algorithms which estimate missing data (as documented in the following Related Works section), few of these take advantage of the time and space dependencies inherent in the WSN datasets during the data imputation process.
%As a result, the accuracy of such approaches is limited.

