\section{Matrix Factorization}  \label{sec:mf}

% need update and to be consistent with related work 
Given a data matrix of observed and missing entries, the essence of Matrix Factorization~(MF) is to learn latent factors from the observed entries, and then leverage these factors to predict the missing entries.
MF is one of the most successful Collaborative Filtering~(CF) models in the area of Recommendation Systems.
In CF, where we consider the rows of the matrix as users and the columns as items, and we wish to predict unknown item ratings by users which reflect known user preferences.
The unknown rating prediction in CF is much like missing data recovery in Wireless Sensor Networks~(WSN), except the rows become time steps, the columns become sensor nodes, and the data characteristic is different. 

In the following discussion, we will introduce the fundamentals of the MF methodology, communicate our novel sensor network data specific modifications to the MF objective function, and finally provide the complete procedure of our proposed method.

\subsection{Basic Formulation of Matrix Factorization}

The approach of MF is strongly related to Singular Value Decomposition~(SVD).
SVD decomposes a completely observed matrix~$\mathbf{R}$ into one diagonal matrix~$\mathbf{D}$ and two unitary matrices~$\mathbf{U}$ and~$\mathbf{V}$ such that
\begin{equation*} \mathbf{R} = \mathbf{U}\mathbf{D}\mathbf{V}^T. \end{equation*}
The largest $K$ singular values, $\mathbf{U}_K \mathbf{D}_K \mathbf{V}_K^T$, form the best $K$-rank approximation of $\mathbf{R}$ under the Frobenius Norm. 

Conventional SVD is undefined when the matrix~$\mathbf{R}$ is incomplete.
In contrast to ealier systems which fill in missing entries and make the matrix dense, recent research shows\cite{koren2009matrix} that a better approach is to factorize $\mathbf{R}$ by considering observed entries only.
Utilizing numerical optimization procedures, we generate two small matrices~$\mathbf{P}_{MK}$ and $\mathbf{Q}_{KN}$ that approximate the observed entries of $\mathbf{R}_{MN}$:
\begin{equation*}\mathbf{R} \approx \mathbf{P} \mathbf{Q}.\end{equation*}
If $\mathbf{R}$ is a matrix of temperature readings, then each row of $\mathbf{P}$ represents latent factors of the time step and each column of $\mathbf{Q}$ is the latent factors of the sensor node.

We adopt the biased-MF which includes row and column biases $\mu_m$ and $\mu_n$. % is added for predicting $r_{m,n}$.
In a temperature monitoring system, the row bias can be understood as the average temperature at a given time, and the column bias reflects the average temperature at the location and the systematic bias of the sensor node.
Therefore, we have the prediction $\hat{r}_{m,n} = \mu_m + \mu_n + \mathbf{p}_m \mathbf{q}_n$.
After adding the regularization term, the whole objective function becomes:
\begin{equation*}\begin{aligned}
\frac{1}{2}\sum_{m,n}{(r_{m,n} - \hat{r}_{m,n})}^2 & + \frac{\beta_1}{2}\sum_m{\mu_m^2} + \frac{\beta_2}{2}\sum_n{\mu_n^2}\\
& + \frac{\beta_3}{2}\sum_m{||\mathbf{p}_m||^2} + \frac{\beta_4}{2}\sum_n{||\mathbf{q}_n||^2}.
\end{aligned}\end{equation*}
In the equation, $\mathbf{p}_m$ are the row factors of $\mathbf{P}$ (for time $m$), and $\mathbf{q}_n$ are the column factors of $\mathbf{Q}$ (for sensor node $n$) respectively.
$\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$ are parameters that control the strength of regularization.

\subsection{Temporally-Regularized Matrix Factorization}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth]{TRMF_illustration.png}
	\caption{illustration of TR-MF}
\end{figure}

%cite temporal reference

Although the goals of both CF and WSN data recovery are unobserved entry prediction, the data characteristics are very different.
In typical recommender systems, users and items may each be on the order of millions.
In contrast, for missing WSN data estimation, there are usually a large number of time steps, with much smaller number of sensor nodes.
The inter-sensor correlations may be easily learned from the data by the CF technique, but the temporal correlation is not considered.
In particular, users and items are independent in CF, while WSN time steps have a fixed order, where nearer time steps have a stronger correlation.
With this observation, we define Temporally-Regularized Matrix Factorization (TR-MF) to better fit the characteristic of WSN data. 

As the name may suggest, TR-MF adds a temporal regularization term to conventional MF.
The temporal regularization forces the bias terms and factors of adjacent rows to be similar, which reflects our intuition that readings in adjacent time steps should be similar.
To achieve this effect, we modify our objective function as follows: 
\begin{equation*}\begin{aligned}
&\frac{1}{2}\sum_{m,n}{(r_{m,n} - \hat{r}_{m,n})}^2 + \frac{\beta_1}{2}\sum_m{\mu_m^2} + \frac{\beta_2}{2}\sum_n{\mu_n^2}\\
+& \frac{\beta_3}{2}\sum_m{||\mathbf{p}_m||^2} + \frac{\beta_4}{2}\sum_n{||\mathbf{q}_n||^2}\\
+& \frac{1}{2}\gamma_1\sum{(\mu_m-\mu_{m+1})^2} 
+ \frac{1}{2}\gamma_2\sum{||\mathbf{p}_m-\mathbf{p}_{m+1}||^2}.
\end{aligned}\end{equation*}

\subsection{Spatio-Temporal-Regularized Matrix Factorization}
%A strong argument that directly exploiting spatial information to specify the assumed inter-sensor node correlation may not be as good as directly learning these correlations directly from the sensor observations themselves.
If the location information of the sensor nodes is known, we may add spatial regularization to strengthen the spatial correlation. Much like temporal regularization, spatial regularization is added for each neighboring sensor node pair ($n_i$,~$n_j$).
The objective function becomes: 
\begin{equation*}\begin{aligned}
&\frac{1}{2}\sum_{m,n}{(r_{m,n} - \hat{r}_{m,n})}^2 + \frac{\beta_1}{2}\sum_m{\mu_m^2} + \frac{\beta_2}{2}\sum_n{\mu_n^2}\\
+&\frac{\beta_3}{2}\sum_m{||\mathbf{p}_m||^2} + \frac{\beta_4}{2}\sum_n{||\mathbf{q}_n||^2}\\ 
+&\frac{1}{2}\gamma_1\sum{(\mu_m-\mu_{m+1})^2}
+ \frac{1}{2}\gamma_2\sum{||\mathbf{p}_m-\mathbf{p}_{m+1}||^2}\\
+&\frac{1}{2}\gamma_3\sum{(\mu_{n_i}-\mu_{n_j})^2} 
+ \frac{1}{2}\gamma_4\sum{||\mathbf{p}_{n_i}-\mathbf{p}_{n_j}||^2}.
\end{aligned}\end{equation*}
We call this Spatio-Temporal-Regularized Matrix Factorization (STR-MF). Note that spatial regularization should be included only when appropriate. See Experiment Section \ref{subsec:exp_basic} for further information.

\subsection{Optimization Procedure}
\label{optimation_procedure}
Several MF solving methodologies have been proposed, such as Stochastic Gradient Descent (SGD)\cite{koren2009matrix,chih2008large}, Alternating Least Square (ALS)\cite{koren2009matrix,zhou2008large}, Newton's method\cite{buchanan2005damped} and Wiberg Algorithm\cite{okatani2007wiberg}.
In our work, we utilize SGD for its efficiency and simplicity. 

In SGD, we incrementally update our model by considering one observed entry at a time.
Focused on one observed reading $r_{m,n}$, the objective function of MF is
\begin{equation*} \frac{1}{2}(r_{m,n} - \hat{r}_{m,n})^2 + \frac{\beta_1}{2}\mu_m^2 + \frac{\beta_2}{2}\mu_n^2 + \frac{\beta_3}{2}||\mathbf{p}_m||^2 + \frac{\beta_4}{2}||\mathbf{q}_n||^2.\end{equation*}
Thus, the update equation is\\
\indent	$\begin{cases}
	\mu_m' = \mu_m - \eta_1 ((\hat{r}_{m,n}-r_{m,n}) + \beta_1 \mu_m) \\
	\mu_n' = \mu_n - \eta_1 ((\hat{r}_{m,n}-r_{m,n}) + \beta_2 \mu_n) \\
	\mathbf{p}_{m}' = \mathbf{p}_{m} - \eta_2 ((\hat{r}_{m,n}-r_{m,n})\mathbf{q}_{n} + \beta_3 \mathbf{p}_{m})\\
	\mathbf{q}_{n}' = \mathbf{q}_{n} - \eta_2 ((\hat{r}_{m,n}-r_{m,n})\mathbf{p}_{m} + \beta_4 \mathbf{q}_{n})\\
	\end{cases}$\\
After we use every rating in the training set once, we update all $\mu_m$ and $\mathbf{p}_m$ simultaneously according to the temporal regularization:\\
\indent $\begin{cases}
	\mu_m' = \mu_m - \eta_1 \gamma_1((\mu_m-\mu_{m-1})+(\mu_m-\mu_{m+1}))\\
	\mathbf{p}_{m}' = \mathbf{p}_{m} - \eta_2 \gamma_2((\mathbf{p}_{m}-\mathbf{p}_{m-1})+(\mathbf{p}_{m}-\mathbf{p}_{m+1}))\\
	\end{cases}$\\
Then, update all $\mu_n$ and $\mathbf{q_n}$ simultaneously by the spatial regularization:\\
\indent $\begin{cases}
	\mu_{n_i}' = \mu_{n_i} - \eta_1 \gamma_3 \sum_{n_j}{(\mu_{n_i} - \mu_{n_j})}\\
	\mathbf{q_{n_i}}' = \mathbf{q_{n_i}} - \eta_2 \gamma_4 \sum_{n_j}{(\mathbf{q_{n_i}} - \mathbf{q_{n_j}})}\\
	\end{cases}$\\

We summarize our algorithm in Procedure \ref{alg:STRMF} and provide details in the following sections.

\paragraph*{Data Normalization}

Unlike the rating scores in CF which have a fixed range, readings from WSN are real-valued, and the range may vary with situation.
Before training, we normalize the training set by its mean and variance, and in training we update the model by the normalized training set.
While predicting the validation set and testing sets, we restore the prediction to the original mean and variance range and then calculate the error.

There are two benefits to performing data normalization.
When the global mean becomes zero, the origin of our model becomes a good initial point.
This initial point impacts the performance of MF greatly because the MF objective function finds only a local minimum (i.e.\ the formulation is non-convex).
Secondly, normalization forces different data sets to look similar which simplifies the parameter tuning task for (S)TR-MF and is the foundation of Multivariate (S)TR-MF (Section \ref{subsec:Multivariate_TRMF}). 


\paragraph*{Initialization}

There are three ways to initialize the bias terms:
\begin{enumerate}
	\setlength {\itemsep}{-5pt}
	\item Set all of them to zeros.
	\item Set the row bias as the row mean, and then set the column bias as the column mean of every reading minus its row bias.
	\item Opposite procedure as previous.
\end{enumerate}
For all components of every $\mathbf{p}_{u}$ and $\mathbf{q}_{i}$, we set them to ~$\mbox{random}(-0.05,0.05)/K$, where $K$ is number of factors.

\paragraph*{Parameters}

We seem to have many parameters: conventional regularization for user biases~$\beta_1$, item biases~$\beta_2$, user factors~$\beta_3$, item factors~$\beta_4$, temporal regularization for biases~$\gamma_1$ and for factors~$\gamma_2$, spatial regularization for biases~$\gamma_3$ and for factors$~\gamma_4$, learning rate for biases~$\eta_1$ and for factors~$\eta_2$, number of factors~$K$, but in many cases we can achieve reasonable performance by setting 
\begin{equation*}\beta_1 = \beta_2 = \beta_3 = \beta_4, \gamma_1 = \gamma_2, \gamma_3 = \gamma_4, \eta_1 = \eta_2. \end{equation*}
Which we use as the setting in our experiment.
We share our experience about setting parameters in Section \ref{subsec:parameter}.

\paragraph*{Stopping Criterion}

To decide when to cease updating the MF model, we use the validation set and cease training when the validation error is increasing and the current iteration number minus the best iteration number is greater than a threshold (we use an iteration threshold of $500$). 

\begin{algorithm}
	\caption{(Spatio-)Temporally-Regularized MF}
	\label{alg:STRMF}
	\textbf{Parameter:} $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, $\gamma_1$, $\gamma_2$, $\eta_1$, $\eta_2$, $K$\\
	\textbf{Input:} training set, validation set
	\begin{algorithmic}
		\State Normalize the training set as $\mathcal{D}$
		\State Initialize $\mu_m$, $\mu_n$, $\mathbf{p}_m$, $\mathbf{q}_n$ for all $m$, $n$
		\Repeat
			\State \textbf{for} each observed reading $r_{m,n}$ in $\mathcal{D}$
				\State \indent Update $\mu_m$, $\mu_n$, $\mathbf{p}_{m}$, $\mathbf{q}_{n}$
			\State Update $\mu_m$, $\mathbf{p}_m$ by temporal regularization
			\State (Update $\mu_n$, $\mathbf{q}_n$ by spatial regularization)
		\Until{stopping criterion is met}
		\State Output the model for testing set prediction
	\end{algorithmic}
\end{algorithm}
