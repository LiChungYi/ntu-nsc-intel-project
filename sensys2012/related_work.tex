\section{Related Work}
 \begin{table*}
\centering
\caption{Methods for Sensory Data Imputation}
\begin{tabular}{|l|l|l|} \hline
Collection&Hot-Deck Imputation&Prediction Models\\ \hline
\multirow{2}{*} {Temporal} & Last-seen\cite{Granger:lastseen} & Linear Interpolation\\ 
& Mean&\\ \hline
\multirow{3}{*}{Spatial}& WARM\cite{le2005estimating}& DEPM\cite{li2008data}\\ 
&FARM\cite{Gruenwald:FARM}&K-NN\cite{pan2010k}\\ 
&advWARM\cite{jiang:assoication}&\\\hline
\multirow{3}{*}{Hybrid Spatio-Temporal}& &DESM\cite{li2008data}\\ 
&STI\cite{Jian-Zhong:STI}&AKE\cite{pan2010k}\\
&&Ensemble STI\cite{Lim:robust} \\\hline \end{tabular}
\end{table*}
Missing data imputation is the process used to fill the missing data items by determine or assign values\cite{Little:hotdeck}. In wireless sensor network, data is often missing as an entire packet due to communication or power exhaustion problem. Moreover, the physical characteristic of WSNs has shown that there are strong spatial and temporal correlation between sensor nodes and its past historical readings\cite{akyildiz2004exploiting}. Besides, sensor mote is consist of different types of sensors, and some sensors, according to their physical phenomenon, have multivariate correlations\cite{lou:multivariate_gap}. In the research above, they build a probabilistic model to estimate the missing values exploiting spatial, temporal, and multivariate correlation conditioned on the observations. 

According to the strategy deployed to fill in the missing values,  sensory data imputation method can be partitioned into hot-deck imputation, and prediction models\cite{Garcia:KNNreview}. 
Hot-deck imputation methods directly replace the value from either neighbourhood values or historical record from itself such as the \textit{last-seen} method. 
The prediction models provide a function to estimate the missing value using the historical data. 
%\begin{equation}
%y_{it} = f( y_{i(t-1)} , y_{j(t-1})
%\end{equation}

Table 1 summarize the sensory data imputation methods. 
The vertical dimension in table 1 shows whether it exploits the temporal, spatial, or hybrid hybrid of both information. 
The horizontal dimension classifies the methods by how the missing value is produced, either direct substitution or through prediction. 

\subsection{Predictions based on Temporal Information }
\subsubsection{Linear Interpolation}
Linear interpolation(LIN) is a common temporal correlation based technique for missing data prediction. 
It is usually regarded as a baseline method, and has been implemented into some data analysis tools. Mathematically, the estimated value $\hat{y_{it}}$ is:
\begin{equation}
\hat{y_{it}} = y_{iu} + \frac{y_{iv}-y{_iu}}{T_v-T_u}(t-T_u)
\end{equation}
where $y_{iu}$and $y{iv}$ are the previous and the next observations for sensor $i$ at time $t$. 
Linear interpolation method does not considers correlation among sensors.
In general, its performance is acceptable when the missing data are scattered. 
However, the quality of imputation goes down significantly if the sampling rates is low or there are consecutive data missing. 
\subsubsection{Last-seen and moving average}
Imputation based on the last-seen, moving average, and other statistics methods apply on the target sensors generally belong to the temporal-based method as well.
These methods share the advantage of comprehensible and easy implementation. 
However,their performance depends on the sensor data missing pattern as well as sampling rates. For higher sampling rates, these methods perform better.   
\subsection{Prediction based on spatial information}

\subsubsection{Data Estimation using Physical Model}
The physical phenomenon of spatial correlation in sensors measurement can be divided into two categories, the point source and the field source\cite{viran:spatialtemporal}. For point source sensors, the relation between the sensors in a region satisfies physical laws in some event detecting sensors such as light ambient sensors and fire detector. 
Their prediction values are the result of linear superposition by neighbourhood sensors. 
In field source sensor applications, the physical phenomenon is dispersed over the field, for example, the temperature and seismic observation. 
Data Estimation using Physical Model(DEPM)\cite{li2008data} employs the basic law of Physics.
They set up the prediction model,  
\begin{equation}
I_K =\sum_{j=1}^M\frac{P_j}{4\pi d^2(I_j,s_k)}
\label{DEPM}
\end{equation} 
$I_k$ is the intensity of target sensor, $j$ is the neighbour sensors, and P stands for the power radiated from two sources. 
They build models of the imputation predictor according to the physical phenomenon, and solve the set of linear inhomogeneous equations to provide an accurate estimation for the light ambient sensors. However, it requires the precise  three-dimensional distance between two sensors, therefore it is not realistic in large sensors deployment.
\subsubsection{Association Rule Mining Estimation}
 Researchers also propose to predict missing values based on the data mining techniques. Window Association Rule Mining(WARM)\cite{le2005estimating} and Freshness Association Rule Mining(FARM)\cite{Gruenwald:FARM} studied the estimations of missing data based on the association rules between spatially correlated neighbours. For example, given sensor $i$ and $j$, where $i$ is the target sensor and $j$ is used to predict the value of $i$, and the state=\{M,L,H\} in the traffic loading sensors. The user-defined \textit{minsupport} is $7/13$. The \textit{actualweightsupport} which is computed from using the rule set and the weighting function (eq\ref{weight_fun}) by sensor $j$ reports on state M is:\{$S_{j} \rightarrow S_{i} \mbox{wrt }  M = 9/13$\}. After  this step, state M will put into the eligible state pool which is later used to do the final prediction. 
 \begin{equation}
 w(n) = p^{n-1}
 \label{weight_fun}
\end{equation}
In equation\ref{weight_fun},  where n means n frames away from the current time, and p stands for the user-defined dumping factor. The final estimation is done by add the score computed by the corresponding rules of each neighbour sensors and their weight in accordance.
 It has the advantage of handling categorical sensor data, but the performance is limited in continuous sensor data streams due to the limitation of association rules. Moreover, the support, confidence of the mined rules need to be predefined by the users which can cause difficulties for building a satisfying model by users without profound knowledge about the environment of deployment. 
\subsubsection{K-Nearest Neighbours Imputation} 
 K-nearest neighbours algorithm(KNN) is a intuitive spatial correlation based imputation method. It has been adopted to estimate the missing value of DNA micro-arrays\cite{Troyanskaya:DNAKNN}. However, it only directly uses the weight average of other genes to fill the missing data. While in WSNs, the sensor data of different nodes is more likely to have some functional relations other than just using sensor values. The two major drawbacks of KNN-based model that predicts only from the spatial correlation are: first of all,  they cannot capture temporal correlation, and the second reason is that most of these methods rely on the knowledge of distance between each sensor, which might not be available or accurate in the real world.
\subsubsection{Multiple Imputation}
The multiple imputation(MI) procedure\cite{Little:hotdeck} imputes the missing data by replaces each missing value with a set of plausible values instead of filling in a single value. Various methods including multiple linear regression, propensity score method, and Markov chain Monte Carlo method are are reported and used in MI. The missing data are filled in M times(M=3-10,\cite{Little:hotdeck}) to generate M complete data sets, and the results from all the M generated data sets are combined for the inference\cite{yuan2000multiple}. 
However, this method is critized on various grounds by WSNs imputation researchers\cite{jiang2007estimating} \cite{Gruenwald:FARM}. First, in sensor data stream, we do not know how many rounds of information shall we use in order to get the associated information.
In addition, it is difficult to draw a pool of similar complete cases for a certain round of a certain sensor. 
Last, it consumes unnecessary time since the sensor data may or may not related to all of the available information.



\subsection{Hybrid Spatio-temporal Imputation}    
According to past studies, algorithms that only use either spatial data or temporal data inevitably face problems\cite{Lim:robust}. In the environment when the radio transmission quality is poor or the packets collision happens frequently, exploiting only temporal information such as the linear interpolation is not ideal, like the linear interpolation. In contrast, for certain sensing applications like light sensor(point source), the spatial information is often unreliable since more than one light source can be sensed. The predictor using only spatial data can easily be harmed by the bias and drift from neighbourhood sensing data. Therefore, applying both temporal and spatial information on the sensory missing data problem seems inevitable. 
Below we will introduce several such methods, and most of them rely on spatial correlation more than temporal correlation, and therefore it requires the knowledge of mutual distance between sensors.     
\subsubsection{Spatial temporal imputation-STI}
An algorithm that uses spatial and temporal imputation(STI) is proposed in \cite{li2008spatial}. 
When a packet is missing, the STI first checks if a neighbour node is within the missing sensor's sensing region. 
The average of the neighbours in the range is then used to impute the missing value. 
Otherwise, the last seen value of the missing sensor is used for prediction. 
The method prefers spatial prediction more than temporal. 
It has the advantage in restricted computing resources, the algorithm is simple and fast and it does not need extra memory to store information other than their mutual distance.
However, problems can arise when the last seen value is far away from the current time stamp, and when most of the nodes in the sensing region has high missing rate. 
Furthermore, the usage of circular region is problematic as most of the landforms might not be uniformly distributed. Finally, determining the range of the radius is tricky, as a larger radius can cause noise while a smaller one cannot highlight the effectiveness of this model(i.e. the results will be similar to linear interpolation).
%The radius of the region is sometimes too large for sensors whose correlations with the target sensor are low in the sensing range.  
\subsubsection{Data Estimation using Statistical Model}
Another algorithm which replying more on spatial more temporal is the Data Estimation using Statistical Model (DEPM)\cite{li2008data}.  
The prediction value of sensor $i$ is:
\begin{equation}
\hat{y_{it}} = (1-\alpha)\hat{y_{i(t-1)}} + (\alpha)\hat{z}
\end{equation}
where$\alpha$ is the Pearson correlation coefficient between the sensor $i$ and the nearest sensor $j$.
The above equation assumes that the $X_i$ and $X_j$ have the similar data fluctuation trend which is true if they are very close to each other. 
$\alpha$ is expected to have more impact on the prediction if $X_i$ is more correlated with $X_j$. 
$\hat{y_{it}}$ is the last estimated value, which measures the influence of the historical sensing data on the current value at node i. 
$\hat{z}$ is the estimation of sensor $j$ based on the observation from sensor $i$ at the time $m$(eq\ref{z-hat}). It measures the influence of the data sensed by the active node i on the data of the nearest node j.  
\begin{equation}
\hat{z} = X_{j(m)}(1+\frac{X_{i(m+1)}-X_{(im)}}{X_{i(m)}})
\label{z-hat}
\end{equation}
The computation in DESM is simple and only little amount of memory is required to store the id of the nearest sensor. 
Note that the temporal feature holds a very small portion of contribution to the prediction process and it often contradicts the nature that the temporal correlation itself is usually higher than any spatial correlation with other sensors. The performance may becomes bad if the nearest sensor is not the most correlated one, i.e. blocked by a wall.
 \subsubsection{Applying K-nearest neighbour Estimation}
Pan\cite{pan2010k} propose the Applying K-nearest neighbour Estimation algorithm(AKE) to exploit the spatial correlation in the missing sensory data problem. They argue that the past research on sensor imputation(STI) estimates the missing problem based on more temporal correlation than spatial correlation(STI). In their observation, the sensor data in WSN often changes rapidly and sharply especially in outside environment. The AKE adopts the linear regression model eq.\ref{ake_j},
\begin{equation}
\hat{y}_{it} =\hat{\alpha} +\hat{\beta}\cdot y_{jt}
\label{ake_j}
\end{equation}
 where $it$ means the time t of sensor i,  and j is its neighbour nodes.  
They claim that using a linear combination of the estimation from neighbour sensors can lower the random error caused by only using a single sensor. 
For given node $i$ whose data is missing at time $t$, and the m nearest neighbour sensors who have values at time $t$[$y_{1t}, y_{2t},\dots, y_{mt}$], the missing value is predicted by :   
 \begin{equation}
 \hat{y}_{it} =\sum_{j=1}^m w_j \cdot \hat{y}_{it}^{(j)}
 \label{ake_impute}
 \end{equation}
% First they build a matrix 
%\[ \left( \begin{array}{cccc}
%
%\end{array}
%\right)
%\]
In eq\ref{ake_impute}, the estimated $\hat{y}_{it}^{j}$ is computed by node $j$ using the coefficients of [$i,j$]. In designing the weighting function, the AKE gives higher correlation pairs a larger weight. They adopts the r-square statistics to rank their correlations due to when sensors have the higher $R^2$, they have the better regression equation fit. 
%Based on the (eq\ref{ake_j}), the $R^2$ is computed by:
%\begin{equation}
%R_(j)^2 = \sum_{n=1}^{h} \frac{(\hat{y}_{in}^(j)-\bar{y}_i)^2}{(y_{in}-\bar{y}_i)^2}
%\end{equation}
Previous study [2] has showed that for high sampling rates sensing, the temporal correlation is higher than the spatial correlation from nearby sensors. 
Therefore, if the consecutive missing frames are within the threshold learned by AKE , the linear interpolation is used to predict the value. 
However, in some environment, the distance used to decide the m nearest neighbours is not feasible. 
Moreover, in certain scenarios where two far away sensors have higher correlation, i.e. two sensors place under the same air-conditioning tuyere, the AKE will ignore the far away sensor.
Finally, the preference on spatial over temporal is also being challenged when the sampling rates is very high, i.e. the body sensors, using temporal linear interpolation has better prediction.   
\subsubsection{Imputation Method-ImM}
 In \cite{Lim:robust}, an imputation method(ImM) consists of two temporal and one spatial predictors was introduced.  The algorithm argues that it is not robust to set the preference for its spatial predictor over temporal predictor, or vice versa.
They assume that the predictor which yields the most accurate results in the training data set is used to predict the missing packets. 
The first temporal predictor(LRS) uses the last transmitted packet as the predicted value eq\ref{lrs}.
\begin{equation}
\hat{y}_{t} =y_{t-1} \label{lrs}
\end{equation} 
 The second temporal predictor uses a linear predictor(LP). Given $o$, the training samples of specific sensor $p$, and $O$ who is the training data of sensors other than $p$. The coefficients of the linear predictor is decided by autocorrelation method of autoregressive modelling. The predicted value by eq\ref{lp}:
\begin{equation}
\hat{y}_{t}^p = a(2)X_o(t-1) - a(3)X_o(t-2)- \dots -a(p+1)X_o(t-p) \label{lp}
\end{equation} 
where $p$ is the sensor node ID.  
The spatial predictor uses the linear predictive model, given the nodes who are successively transmitted to the sink node, $p_1,p_2,\dots,p_L$, the predicted value is(eq\ref{ImM_Spatial}): 
\begin{equation}
\hat{y}_{t}^p = \sum_{l=1}^L a_lp_l
\label{ImM_Spatial}
\end{equation}
 where the weights is determined by weighted coefficients(WC) using least square method$[a_1,a_2,\dots,a_L]^T = (O^TO)^{-1}O^To$.
However, the performance of the ImM may degrades if the changes in the continuous data stream are sharp in the high sampling rate data set. 
For example, the wind sensors in the windy climate area may affect the method applied on the target sensor.  
\subsection{Review of Matrix Factorization  }
Matrix factorization has many applications in computer visions, recommendation system\cite{chen2011linear}, and data imputation. Their goals are to factorize the measurement matrix R and find the two subspace: U and V. We will introduce two applications other than WSNs data imputation in the following sections. The first application is the EOF-based prediction method in satellite photography and the second one is the Structure-from-motion(SFM) in computer vision.
\begin{equation}
M_{d\times n}= U_{d\times k}V^t_{k\times n}
\end{equation}
\subsubsection{Prediction based on Empirical Orthogonal Functions }
Empirical Orthogonal Functions(EOF) has been applied to oceanographic application to solve the problem of missing or unreliable satellite data\cite{beckers2003eof}. 
They propose the DINEOF (Data Interpolating Empirical Orthogonal Functions) which is an Singular value decomposition(SVD)-based technique for the reconstruction of missing data.
In their method, given a matrix $X_{m*n}$ with its entry $(X)_{ij}$ which contains the observations values at $i$ and moment $j$. 
in eq\ref{EOF}, $X_\gamma$ is the matrix whose items are zero except the missing data points.
$U_N$ is the matrix constructed by the first $N$ spatial EOFs, which have the same meaning as $N$-modes in SVD. 
In their method, all the missing values are filled with zero first, then the prediction of DINEOF becomes eq.\ref{EOF}
\begin{equation}
(X_\gamma)_{ij} = (U_ND_NV_N^T)_{ij}
\label{EOF}
\end{equation}
The operations stop when $X_{ij}$ is filled by all the missing item with their prediction in $(X_\gamma)_{ij}$. 
Researchers of \cite{alvera2009enhancing} point out that when large gap of data missing due to clouds or rains, the remained observations may not be representative of the whole spatial variability of the domain. Therefore they apply a Laplacian filter $F$ on the terms of $X$ before the estimations of EOFs. After filtering, two consecutive images in matrix $X$ with a large time gap between them are less related than two consecutive images. 
\begin{eqnarray}
&&\tilde{B} = F^TBF \\
&&B=X^TX
\label{enhance_eof}
\end{eqnarray} 

However, there are several drawbacks in the SVD-based methods when apply to impute large-scale sensors network. First, SVD needs to fill all the missing values before factorization which can be very expensive as it significantly increases the amount of data. The inaccurate imputation might also distort the data considerably\cite{koren2009matrix}. In addition,\cite{ke2005robust} point out SVD is sensitive to outliers and missing data. 
  
\subsubsection{Matrix Factorization on Structure of Motion} 
Structure from motion(SFM)\cite{tomasi1992shape} and motion estimation are among the most important application in computer vision.   
A number of studies have investigated different approaches to the MF-based structure from motion problem. \cite{buchanan2005damped} performs the L2-norm minimization using the standard Newton algorithm with a damping factor. In \cite{deguchi2011efficient}, they propose a new method incorporates the damping factor into the Wiberg method to solve the L2-norm problem. A L1-norm factorization using Wiberg method is proposed in\cite{eriksson2010efficient}. They turn the objective function into a non-linear least square problem and adopt Gauss-Newton method to solve the problem. 
However, the size and ranks of the video data is different from the long-term and densely deployed sensor data. 
The rank of linear subspace in SFM has rank of three after it is subtracted by its column-wise mean. It is also noted that the video camera in essential is an univariate sensor device which is different from the nature of heterogeneity of sensor networks. 