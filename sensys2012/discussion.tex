\section{Discussion}  \label{sec:disc}

\subsection{Hybrid Temporal/Spatial Imputation Methods}

Hybrid methods of temporal and spatial approaches are less common in the literature.
For example, the average of the temporal approach of linear interpolation and the spatial approach of multivariate regression has been reported[8].
Strictly speaking, this approach can be thought of as an ensemble approach between the two methods rather than a fully-integrated approach which considers both temporal and spatial aspects of WSN data.

\subsection{Parameter Setting} \label{subsec:parameter}

For both MF and TF model, we observe that the parameter setting in random split must be different from that in temporal split. Basically, we need stronger regularization in temporal split.  We provide some details and explanation here. 

Let us look at two illustrating examples.
Here we ignore bias terms for simplicity.
Random split is like Equation \ref{randomSplit_matrix}.
We have more confidence in our prediction and we rely mostly on temporal correlation, so we prefer more factors and weak conventional regularization.
We also see that linear interpolation can be viewed as a solution of TRMF, in which the number of factors is large and no conventional regularization is applied. 
\begin{equation}
\label{randomSplit_matrix}
\begin{bmatrix}
16 & 22 & 18\\
 ? & 22 & 19\\
18 &  ?	& 22\\
19 & 24 &  ?\\
20 & 26 & 26\\
\end{bmatrix} 
= 
\begin{bmatrix}
16 & 22 & 18\\
\mathbf{17} & 22 & 19\\
18 & \mathbf{23}	& 22\\
19 & 24 & \mathbf{24}\\
20 & 26 & 26\\
\end{bmatrix} 
\times
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
\end{bmatrix} 
\end{equation}

Temporal split is like Equation \ref{temporalSplit_matrix}.
In contrast to temporal split case, we can't use temporal correlation to infer the missing values and we have less confidence in prediction.
We can only rely on the spatial correlation, so we prefer fewer factors and stronger conventioal regularization.

\begin{equation}
\label{temporalSplit_matrix}
\begin{bmatrix}
30 & 30 & 28\\
31 & 31 & 28\\
32 &  ? & 28\\
33 &  ? & 28\\
32 &  ? & 28\\
\end{bmatrix} 
= 
\begin{bmatrix}
30 & 28\\
31 & 28\\
32 & 28\\
33 & 28\\
32 & 28\\
\end{bmatrix} 
\times
\begin{bmatrix}
1 & 1 & 0\\
0 & 0 & 1\\
\end{bmatrix} 
\end{equation}

To be specific, we share the actual values of the parameters.
The learning rate $\eta$ is $0.04$ to $0.004$ for MF and $0.001$ to $0.0001$ for TF, which is chosen as the largest value that still maintains stable optimization process.
Temporal regularization times learning rate ($\gamma \times \eta$) is set to $0.2$ in all of our experiment.
The conventional regularization is $0.001$ for temporal split and $0$ for random split in MF, while in TF, it is $0.005$ for temporal split and $0.001$ for random split.
Finally, we observe that a large $K$ (although slower) doesn't lead to overfitting because SGD is a natural regularization that prefers small factor values. 
The number of factors $K$ is $54$ for MF and $30$ for TF in Berkeley data set, and it is $21$ for MF and $10$ for TF in traffic data set. 


\subsection{Time Complexity}
Suppose the number of factors is $K$.
For MF (TR-MF and STR-MF), the number of time steps is $F_1$ and the number of sensor nodes is $F_2$. 
For TF, the numbers of three features are $F_1$, $F_2$, $F_3$.  
If there are $R$ regularization terms (spatial and temporal), and there are $N$ observed entries from the training set, then the time spent in each update is $\Theta(KN + KR)$ (independent of $F_1$, $F_2$ and $F_3$)for both MF and TF. 
The total iteration number varies with data and its missing rate. Note that although TF sounds more complicated, it runs actually faster than MF, since its $K$ is smaller and it converges faster.
However, the training time is not the main concern in our application, since we only need to train our model once.
%The total iteration depends greatly on the data and the actual running time may 

The imputation time is more important as we need the values for further learning task like event classification.
Both MF and TF are very efficient in the testing phase.
To predict a missing value, it takes $2K$ multiplication operations to MF and $3K$ multiplication operations to TF.
Since $K$ is very small so the prediction can be done in real time.
On a normal laptop, it takes less than a second to predict all values for our Berkey data matrix ($270000$ values).

%\begin{comment}
%In the training phase, TRMF update factors 2NK times and update biases 2N times. After each iteration, the time regularization terms are updated once. So the complexity of a iteration is $\mathbf{O}(NK+N+SK+FK)$. In testing phase, TRMF cost  
%$\mathbf{O}(K)$ to predict a missing value.\\
%Suppose the factor size of TF model is K, $F_1$, $F_2$, $F_3$ represent the number of three features use in TF. In the training phase. In the training phase, TF update factors 3NK times and update biases 3N times. After each iteration, the time regularization terms are also updates once. The complexity of a iteration is $\mathbf{O}(NK+N+F_1K+F_2K+F_3K)$. In the testing phase, TF also cost  $\mathbf{O}(K)$ to predict a missing. However,we found if you want to get the equally performance between TF and TRMF, the parameter $K$ of TF is usually smaller than TRMF.
%\end{comment}
