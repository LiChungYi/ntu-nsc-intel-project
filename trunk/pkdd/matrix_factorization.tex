\section{Using Matrix Factorization for Imputation}  \label{sec:mf}

% need update and to be consistent with related work, need to add the Netflix and our KDD2011 papers to support MF

Matrix Factorization (MF) is arguably the most successful Collaborative Filtering~(CF) technique in the area of Recommendation Systems~\cite{koren2009matrix, chen2011linear}. Compared with other recommendation models such as regression-based prediction models, graph-based random walk models, or simple statistical models, the MF model possesses the advantages of being more accurate and more scalable to data size.
A key feature of MF models is its capability to learn {\em latent} factors from {\em relatively sparse} observations, and to leverage these factors to impute the missing elements in the matrix.
In the following, we will first introduce the fundamentals of the MF methodology, present our novel sensor-data-specific 
modifications to the MF objective function, and finally provide the complete training procedure of the proposed method.

\subsection{Introducing Matrix Factorization}

%, a widely known dimension reduction technique, 
%in dealing with incomplete or sparse data.
%The goal of SVD is to decompose a fully observed matrix~$\mathbf{R}$ into one diagonal matrix~$\mathbf{D}$ and two unitary matrices~$\mathbf{U}$ and~$\mathbf{V}$ such that
%%\begin{equation*} 
%$\mathbf{R} = \mathbf{U}\mathbf{D}\mathbf{V}^T.$ %\end{equation*}
%The largest $K$ singular values and vectors, $\mathbf{U}_K \mathbf{D}_K \mathbf{V}_K^T$, form the best $K$-rank approximation of $\mathbf{R}$ under the Frobenius Norm. Unfortunately, conventional SVD cannot be performed when the matrix~$\mathbf{R}$ is incomplete. 
%Therefore, to exploit latent dimensions through SVD for imputation, prior work (such as EOF) has to fill in missing entries first 
%(usually using zeros or an averaged value) and then perform dimension reduction through retaining the largest $K$ singular values~\cite{beckers2003eof}. 
%Such strategy does create some dilemmas as a high-quality imputation model is needed in the first stage to guarantee the quality of 
%data reproduced in the later stage, but it is such high-quality that we seek for data imputation. In a nutshell, SVD, although 
%proven to be ideal for dimension reduction, can hardly be applied to sparse matrices where a significant fraction of 
%the data are missing.
%
As stated in section \ref{sec:rw_spatial_temporal}, conventional SVD is not appropriate for the task of missing data imputation.
Matrix Factorization is designed to amend the limitations of SVD, and recently researchers have shown \cite{koren2009matrix} that the matrix factorization model is indeed a better approach to learn the latent factors given sparse matrices, because during the factorization procedure only the observed entries are exploited.
Utilizing numerical optimization procedures, for a partially observed matrix~$\mathbf{R}$, MF produces two latent 
matrices~$\mathbf{P}_{M \times K}$ and $\mathbf{Q}_{K \times N}$ whose multiplication seeks to approximate the observed entries in $\mathbf{R}_{M \times N}$:
\begin{equation*}\small \mathbf{R} \approx \mathbf{P} \mathbf{Q}.\end{equation*}
Given $\mathbf{R}$ being the sensor network readings, each row of $\mathbf{P}$ represents latent factors in the temporal dimension and each column of $\mathbf{Q}$ represents the latent factors in the dimension of correlation among sensors.

We adopt a biased-MF that includes row and column biases $\mu_m$ and $\mu_n$. 
In a temperature monitoring system, the row bias can be understood as the average temperature at a given time, and the column bias reflects the average temperature at the location plus the systematic bias of the sensor node.
The predictions of missing values can be obtained through $\hat{r}_{mn} = \mu_m + \mu_n + \mathbf{p}_m \mathbf{q}_n$.
After adding the regularization term to constrain the scale of latent factors, the objective function of MF becomes to minimize:
\begin{equation*}\small \begin{aligned}
\frac{1}{2}\sum_{m,n}{(r_{mn} - \hat{r}_{mn})}^2
+ \frac{\beta}{2}(\sum_m{(\mu_m^2 + ||\mathbf{p}_m||^2)} + \sum_n{(\mu_n^2 + ||\mathbf{q}_n||^2)}),
\end{aligned}\end{equation*}
where $\mathbf{p}_m$ are the row factors of $\mathbf{P}$ (for time $m$), and $\mathbf{q}_n$ are the column factors of $\mathbf{Q}$ (for sensor node $n$) respectively.
$\beta$ is the parameter that controls the strength of regularization.

\subsection{Temporally-Regularized Matrix Factorization (TR-MF)}

Although we map the sensor data imputation task to a CF-based recommendation task, there are indeed some major differences in the properties of the data.
Normal CF or MF models assume no ordering on the users (i.e., temporal dimension). That is, we can randomly swap the rows in the matrix without affected the factorization outcome. However, such independence does not exist in sensor data as a 
sensor's signal in time $t$ is highly dependent on that of time $t-1$. In other words, an ideal model should consider such ordering dependency, and under such a model reordering the
rows would significantly affect the outcome of factorization.


%\vspace{-0.5cm}
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.6\textwidth, trim=4 4 4 4, clip]{TRMF_illustration_white.png}
%	\vspace{-0.1in}
%	\caption{\label{fig:tr-mf} Illustration of TR-MF}
%\end{figure}
%\vspace{-0.5cm}

With this observation, we propose a Temporally-Regularized Matrix Factorization (TR-MF) to better model the characteristic of sensor data. As the name may suggest, TR-MF adds a temporal regularization term to conventional MF. 
%(see Figure~\ref{fig:tr-mf})
The temporal regularization forces the latent factors of adjacent rows to be similar, which reflects the fact that 
readings in adjacent time steps should be similar. We also add a similar regularization term for 
adjacent row biases.
The modified objective function ($\gamma$ controls the strength of the regularization) looks like: 
\begin{equation*}\small \begin{aligned}
\frac{1}{2}\sum_{m,n}{(r_{mn} - \hat{r}_{mn})}^2
&+ \frac{\beta}{2}(\sum_m{(\mu_m^2 + ||\mathbf{p}_m||^2)} + \sum_n{(\mu_n^2 + ||\mathbf{q}_n||^2)}) \\
&+ \frac{\gamma}{2} \sum_m{((\mu_m-\mu_{m+1})^2 + ||\mathbf{p}_m-\mathbf{p}_{m+1}||^2)}.
\end{aligned}\end{equation*}

\subsection{Spatio-Temporal-Regularized Matrix Factorization (STR-MF)}
%A strong argument that directly exploiting spatial information to specify the assumed inter-sensor node correlation may not be as good as directly learning these correlations directly from the sensor observations themselves.

Although previously we argued that nearby sensors might not necessarily possess the highest correlation with each other, 
here we would like to show that our TR-MF model can easily be extended to accommodate spatial correlation if one decides to do so. 
Given the distance (or any kind of `closeness' measure) between sensors, we can add spatial regularization terms to bias for 
possible spatial correlation.
The objective function ($\gamma_s$ controls the strength of the regularization) then becomes: 
\begin{equation*}\small \begin{aligned}
\frac{1}{2}\sum_{m,n}{(r_{mn} - \hat{r}_{mn})}^2
&+ \frac{\beta}{2}(\sum_m{(\mu_m^2 + ||\mathbf{p}_m||^2)} + \sum_n{(\mu_n^2 + ||\mathbf{q}_n||^2)}) \\
&+ \frac{\gamma}{2} \sum_m{((\mu_m-\mu_{m+1})^2 + ||\mathbf{p}_m-\mathbf{p}_{m+1}||^2)}\\
&+ \frac{\gamma_s}{2} \sum\limits_{\substack{n_i, n_j \\ \text{neighbors}}}{((\mu_{n_i}-\mu_{n_j})^2 + ||\mathbf{p}_{n_i}-\mathbf{p}_{n_j}||^2)}.
\end{aligned}\end{equation*}
We call this Spatio-Temporal-Regularized Matrix Factorization (STR-MF). 
Users should exploit spatial regularization with care, however, as our experimental study will show 
that biasing for spatial correlation can often produce inferior results. 
In the following discussion, we will focus mainly on TR-MF.

\subsection{Optimization Procedure}
\label{optimation_procedure}
Several methods to learn MF have been proposed, such as Stochastic Gradient Descent (SGD)\cite{koren2009matrix,chih2008large}, Alternating Least Square (ALS)\cite{koren2009matrix,zhou2008large}, Newton's method\cite{buchanan2005damped} and Wiberg Algorithm\cite{okatani2007wiberg}.
For sensor data, we suggest SGD for its efficiency and simplicity. 

In SGD, we incrementally update our model by considering one reading at a time.
Focused on one observed reading $r_{mn}$ with the following objective function
\begin{equation*} \small \frac{1}{2}(r_{mn} - \hat{r}_{mn})^2
+ \frac{\beta}{2}(\mu_m^2 + ||\mathbf{p}_m||^2 + \mu_n^2 + ||\mathbf{q}_n||^2).\end{equation*}

It is not hard to derive the TR-MF update equations ($\eta$ controls the learning rate) as \\
\indent	$ \begin{cases} 
	\mu_m' = \mu_m - \eta ((\hat{r}_{mn}-r_{mn}) + \beta \mu_m) \\
	\mu_n' = \mu_n - \eta ((\hat{r}_{mn}-r_{mn}) + \beta \mu_n) \\
	\mathbf{p}_{m}' = \mathbf{p}_{m} - \eta ((\hat{r}_{mn}-r_{mn})\mathbf{q}_{n} + \beta \mathbf{p}_{m})\\
	\mathbf{q}_{n}' = \mathbf{q}_{n} - \eta ((\hat{r}_{mn}-r_{mn})\mathbf{p}_{m} + \beta \mathbf{q}_{n})\\
	\end{cases} $ \\
For each reading, we update all the $\mu_m$ and $\mathbf{p}_m$. Then after a full scan of all observed readings, 
we perform temporal regularization by updating all $\mu_m$ and $\mathbf{p}_m$ simultaneously according to the following equations:\\
\indent $ \begin{cases}
	\mu_m' = \mu_m - \eta \gamma((\mu_m-\mu_{m-1})+(\mu_m-\mu_{m+1}))\\
	\mathbf{p}_{m}' = \mathbf{p}_{m} - \eta \gamma((\mathbf{p}_{m}-\mathbf{p}_{m-1})+(\mathbf{p}_{m}-\mathbf{p}_{m+1}))\\
	\end{cases} $\\
The updating procedure is summarized in Procedure~\ref{alg:STRMF}. Note that we propose to avoid updating the temporal regularization with the update of each reading, because doing so can bias the model toward the time steps that possess fewer missing readings, which contradicts the goal of data imputation. 

\begin{algorithm}[thb]
	\caption{(Spatio-)Temporally-Regularized MF}
	\label{alg:STRMF}
	\textbf{Parameters:} $\beta$, $\gamma$, ($\gamma_s$,) $\eta$, $K$\\
	\textbf{Input:} training set, validation set
	\begin{algorithmic}
		\State Normalize the training set as $\mathcal{D}$
		\State Initialize $\mu_m$, $\mu_n$, $\mathbf{p}_m$, $\mathbf{q}_n$ to small random numbers
		\Repeat
			\State \textbf{for} each observed reading $r_{mn}$ in $\mathcal{D}$
				\State \indent Update $\mu_m$, $\mu_n$, $\mathbf{p}_{m}$, $\mathbf{q}_{n}$
			\State Update $\mu_m$, $\mathbf{p}_m$ by temporal regularization
			\State (Update $\mu_n$, $\mathbf{q}_n$ by spatial regularization)
		\Until{stopping criterion is met}
		\State Output the imputation prediction model
	\end{algorithmic}
\end{algorithm}

For STR-MF, we can update all $\mu_n$ and $\mathbf{q_n}$ simultaneously as:\\
\indent $\begin{cases}
	\mu_{n_i}' = \mu_{n_i} - \eta \gamma_s \sum_{n_j}{(\mu_{n_i} - \mu_{n_j})}\\
	\mathbf{q_{n_i}}' = \mathbf{q_{n_i}} - \eta \gamma_s \sum_{n_j}{(\mathbf{q_{n_i}} - \mathbf{q_{n_j}})}\\
	\end{cases}$\\
where $n_i$ and $n_j$ are a pair of neighboring sensor nodes.

\paragraph*{Data Normalization}
Unlike the ratings in recommendation systems that are normally within a certain range (e.g., 1 to 5 stars), readings from WSNs are real-valued (rounded to a desired level of precision), and the range may vary with the sensors.
Here we propose to normalize the training set to zero mean and unit variance before conducting MF learning, and once the missing values are produced by our model, we need to rescale the values to the original mean and variance.
Although this procedure does not change the quality of outcomes theoretically, in practice we do find some benefits: 
First, when the global mean becomes zero, the origin of our model naturally becomes a fine initial point for MF training, which is very important for non-convex optimization techniques such as MF.
Second, normalization forces different datasets to look similar, which simplifies the parameter tuning task. 


%\paragraph*{Parameters Setting}
%There are several parameters in our model: conventional regularization for user biases~$\beta_1$, item biases~$\beta_2$, user factors~$\beta_3$, item factors~$\beta_4$, temporal regularization for biases~$\gamma_1$ and for factors~$\gamma_2$, spatial regularization for biases~$\gamma_3$ and for factors$~\gamma_4$, learning rate for biases~$\eta_1$ and for factors~$\eta_2$, number of factors~$K$, and it can be time consuming to search for a good combination. 
%Here we provide some empirical suggestions to reduce the search space while still obtaining results with decent quality: the rule of thumb is to setup parameters of similar physical meanings to be identical. For example: the following setup is what we exploited for the experiments:
%$ \beta_1 = \beta_2 = \beta_3 = \beta_4, \gamma_1 = \gamma_2, \gamma_3 = \gamma_4, \eta_1 = \eta_2$. %\end{equation*}
%Advanced discussion on this aspect appears later in Section~\ref{subsec:parameter}.
%The parameters in our model are conventional regularization~$\beta$, temporal regularization~$\gamma$, learning rate~$\eta$, number of factors~$K$.
%A too small~$\eta$ or a too large~$K$ can slow down the training process, but it won't degrade the model.

\paragraph*{Stopping Criterion}
In addition to the training dataset used to learn the TR-MF (or STR-MF) model, we use a validation 
dataset to determine the model parameters and when to cease updating the model. 
More specifically, the training stops when the validation error fails to decrease for a certain 
amount of iterations ($500$ in our experiments). 

\paragraph*{Time Complexity}
While training TR-MF, the time required for each update is $\Theta(KN)$, where $K$ is the number 
of factors ($\leq 54$ in our experiments) and $N$ is the number of observed readings.
% and $R$ is the number of temporal and spatial regularization terms.
In other words, it is independent of the size of the data matrix. 
The total number of iterations varies with data quality, the missing rate and some parameters such as the learning rate and stopping criterion.
In our experiments, it normally takes several minutes to train a TR-MF model on a computer with Xeon 2.53GHZ processor and 16 to 64G memory, but in some circumstances the training time can grow to 1 to 2 hours.
To predict a missing value, it takes $2K$ multiplication operations, which can be done in real time. 
On a normal laptop, it takes less than one second to predict all values for our Berkeley data matrix ($270,000$ values).
Note that the training time is not as important as the prediction time, as normally we need only to train our model once to learn the optimal parameters and factors for prediction.

