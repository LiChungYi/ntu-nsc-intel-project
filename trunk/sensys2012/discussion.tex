\section{Discussion}  \label{sec:disc}

%\subsection{Hybrid Temporal/Spatial Imputation Methods}

%Hybrid methods of temporal and spatial approaches are less common in the literature.
%For example, the average of the temporal approach of linear interpolation and the spatial approach of multivariate %regression has been reported[8].
%Strictly speaking, this approach can be thought of as an ensemble approach between the two methods rather than a fully-%integrated approach which considers both temporal and spatial aspects of WSN data.

\subsection{Parameter Setting} \label{subsec:parameter}

For both MF and TF models, we have realized that the parameter setting in random split experiments has to be different from that in temporal split experiments. In a nutshell, wstronger regularization is needed in temporal split. Below we provide some qualtative analysis using examples. 

%Here we ignore bias terms for simplicity.
Equation 6.1 shows a random missing case and 6.2 shows a continuous missing case. In random missing, the confidence for prediction is usually higher and the model relies more on temporal correlation, therefore it is preferred to use more factors and weaken the strength of conventional regularization (i.e. try to overfit the data). Note that here the term conventional refers to the normal regularization imposing to the latent features, not the additonal ones we proposed. 
As shown in equation 6.1, the linear interpolation model can be viewed as a special-case solution for TRMF, in which the number of factors is large and less regularization is applied. 
\begin{equation}
\label{randomSplit_matrix}
\begin{bmatrix}
16 & 22 & 18\\
 ? & 22 & 19\\
18 &  ?	& 22\\
19 & 24 &  ?\\
20 & 26 & 26\\
\end{bmatrix} 
= 
\begin{bmatrix}
16 & 22 & 18\\
\mathbf{17} & 22 & 19\\
18 & \mathbf{23}	& 22\\
19 & 24 & \mathbf{24}\\
20 & 26 & 26\\
\end{bmatrix} 
\times
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
\end{bmatrix} 
\end{equation}

In contrast to the random missing case, in continuous missing scenario we can't rely too much on temporal correlations because they do not exist. Therefore, the model tries to rely more on the correlations between sensors or spatial information, and is less confident about the prediction it makes. In this case, we want to avoid overfitting by imposing stronger conventioal regularization.

\begin{equation}
\label{temporalSplit_matrix}
\begin{bmatrix}
30 & 30 & 28\\
31 & 31 & 28\\
32 &  ? & 28\\
33 &  ? & 28\\
32 &  ? & 28\\
\end{bmatrix} 
= 
\begin{bmatrix}
30 & 28\\
31 & 28\\
32 & 28\\
33 & 28\\
32 & 28\\
\end{bmatrix} 
\times
\begin{bmatrix}
1 & 1 & 0\\
0 & 0 & 1\\
\end{bmatrix} 
\end{equation}

Below we share the actual values of the parameters that yield the results of decent quality.
The learning rate $\eta$ is $0.04$ to $0.004$ for MF and $0.001$ to $0.0001$ for TF, which is chosen as the largest value that is capable of producing stable and consistent improvement during the optimization.
Also a strong temporal regularization is always preferred.
Temporal regularization is set to 0.2/learning-rate in all of our experiment.
The conventional regularization is $0.001$ for temporal split and $0$ for random split in MF, while in TF, it is $0.005$ for temporal split and $0.001$ for random split.
Finally, we observe that using larger latent features $K$ usually doesn't lead to overfitting because SGD is a natural regularization that prefers small factor values, but the training time becomes longer.
The number of factors $K$ is $54$ for MF and $30$ for TF in Berkeley data set, and it is $21$ for MF and $10$ for TF in traffic data set. 

\subsection{Time Complexity Analysis}
Here we first discuss the complexity in learning MF and TF models, and then we discuss their complexity in prediction.
Let us denote the number of factors as $K$, the number of sampling (i.e. time steps) as $F_1$ and the number of sensor nodes as $F_2$ in TR-MF and STR-MF. For 3rd-order TF, the numbers of three features are $F_1$, $F_2$, $F_3$.  
Assuming there are $R$ spatial and temporal regularization terms, and there are $N$ observed readings from the training set, then the time required for each update is $\Theta(KN + KR)$ (independent of $F_1$, $F_2$ and $F_3$)for both MF and TF.
The total iteration number varies with data quaility, the missing rate, and some paramters such as the learning rate as well as the stopping criteria. 
Note that we would like to emphasize that the training time is not as important as the prediction time, as normally we only need to train our model once to learn the optimal features and paramters for prediction.

The imputation time is more critical as ideally we hope to achieve real-time prediction given a learned model. 
Both MF and TF are very efficient in prediction in thoery and practice.
To predict a missing value, it takes $2K$ multiplication operations to MF and $3K$ multiplication operations to TF.
Since $K$ is relatively small so the prediction can be done in real time.
On a normal laptop, it takes less than a second to predict all values for our Berkey data matrix ($270000$ values).

%\begin{comment}
%In the training phase, TRMF update factors 2NK times and update biases 2N times. After each iteration, the time regularization terms are updated once. So the complexity of a iteration is $\mathbf{O}(NK+N+SK+FK)$. In testing phase, TRMF cost  
%$\mathbf{O}(K)$ to predict a missing value.\\
%Suppose the factor size of TF model is K, $F_1$, $F_2$, $F_3$ represent the number of three features use in TF. In the training phase. In the training phase, TF update factors 3NK times and update biases 3N times. After each iteration, the time regularization terms are also updates once. The complexity of a iteration is $\mathbf{O}(NK+N+F_1K+F_2K+F_3K)$. In the testing phase, TF also cost  $\mathbf{O}(K)$ to predict a missing. However,we found if you want to get the equally performance between TF and TRMF, the parameter $K$ of TF is usually smaller than TRMF.
%\end{comment}
