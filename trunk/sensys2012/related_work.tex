\section{Related Work}\label{sec:rw}


%Missing data imputation is the process used to fill the missing data items by determine or assign values\cite{Little:hotdeck}. The physical characteristic of WSNs has shown that there are strong spatial and temporal correlation between sensor nodes and its past historical readings\cite{akyildiz2004exploiting}. 
%Sensors, according to their physical phenomenon, have multivariate correlations\cite{lou:multivariate_gap}. 
%\end{comment}

Table 1 summarizes the data imputation methods in two dimensions: one describing the information utilized for imputation, and the other describing the imputation strategy. The missing-values filling strategy can be further categorized as hot-deck imputation and prediction models\cite{Garcia:KNNreview}. 
Hot-deck imputation methods directly fill in the missing values using either neighbourhood values or historical records from itself such as the \textit{last-seen} method, while the prediction models use a function to estimate the missing value given the historical data. 


%\begin{equation}
%y_{it} = f( y_{i(t-1)} , y_{j(t-1})
%Table~\ref{tbl:methods} summarize the sensory data imputation methods. The vertical dimension in Table~\ref{tbl:methods} shows whether it exploits the temporal, spatial, or hybrid hybrid of both information. The horizontal dimension classifies the methods by how the missing value is produced, either direct substitution or through prediction. 

\subsection{Predictions based on Temporal Information }
Linear interpolation is a common temporal correlation based technique for missing data prediction. 
It is usually regarded as a baseline method, and has been implemented into some data analysis tools. Mathematically, the estimated value $\hat{y_{it}}$ is:
$\hat{y_{it}} = y_{iu} + \frac{y_{iv}-y{_iu}}{T_v-T_u}(t-T_u)$, where $y_{iu}$and $y{iv}$ are the previous and the next observations for sensor $i$ at time $t$.
Linear interpolation method does not considers correlation among sensors. In general, its performance is acceptable when the missing data are scattered. 
However, the quality of imputation goes down significantly when the sampling rates is low or there are consecutive data missing. 
Imputation based on the last-seen, moving average, and other statistics methods apply on the target sensors generally belong to the temporal-based method as well.
These methods share the advantage of comprehensible and easy implementation. 
However,their performance depends significantly on the sensor data missing pattern as well as sampling rates.   
\subsection{Prediction models using spatial information}
%The physical phenomenon of spatial correlation in sensors measurement can further be divided into two categories, the point source and the field source\cite{viran:spatialtemporal}. 
In some point source sensors where the strength of the sensed signal in a region satisfies certain physical laws. The imputation values can be obtained as the result of linear superposition in neighbourhood sensors. 
For instance, the Data Estimation using Physical Model(DEPM)\cite{li2008data} model employs the basic law of Physics and design a prediction function,  
%\begin{equation}
$I_K =\sum_{j=1}^M\frac{P_j}{4\pi d^2(I_j,s_k)}$
%\label{DEPM}
%\end{equation} 
where $I_k$ is the intensity of the target sensor value, $j$ represents the neighbour sensors, and P stands for the power radiated from one source to another. 
However, such models are only applicable to limited types of signals, and genreally require the precise three-dimensional distance among sensors.

Researchers also propose to predict missing values based on the data mining techniques. Window Association Rule Mining(WARM)\cite{le2005estimating} and Freshness Association Rule Mining(FARM)\cite{Gruenwald:FARM} studied the estimations of missing data based on the association rules among spatially correlated neighbours. 
Such methods the advantage of handling categorical sensor data, but the performance is limited in continuous sensor data streams due to the limitation of association rules. 
%Moreover, the support, confidence of the mined rules need to be predefined by the users which can cause difficulties for building a satisfying model by users without profound knowledge about the environment of deployment. 
%\subsubsection{K-Nearest Neighbours Imputation} 
%  The two major drawbacks of KNN-based model that predicts only from the spatial correlation are: first of all,  they cannot capture temporal correlation, and the second reason is that most of these methods rely on the knowledge of distance between each sensor, which might not be available or accurate in the real world.

The multiple imputation(MI) procedure\cite{yuan2000multiple} imputes the missing data by replacing each missing value with a set of plausible values instead of filling in a single value. Various methods including multiple linear regression, propensity score method, and Markov chain Monte Carlo method are are reported and used in MI. The missing data are filled in $M$ times($M$=3-10,\cite{Little:hotdeck}) to generate M complete data sets, and the results from M generated data sets are combined for the inference\cite{yuan2000multiple}. The final estimate comes from the average of these $M$ values. 
However, this method is critized on various grounds by WSNs imputation researchers\cite{jiang2007estimating} \cite{Gruenwald:FARM}. First, in sensor data stream, we do not know how many rounds of information shall we use in order to get the associated information.
In addition, it is difficult to draw a pool of similar complete cases for a certain round of a certain sensor. 
Last, it consumes unnecessary time since the sensor data may or may not related to all of the available information.



\subsection{Hybrid Spatio-temporal Imputation}    
Past studies indicate that imputation algorithms that only use either spatial or temporal information cannot produce satisfiable outcomes \cite{Lim:robust}. In the environment when the radio transmission quality is poor or the packets collision happens frequently, exploiting only temporal information is not ideal. In contrast, for certain sensing applications (e.g. Figure 1), purely rely on the spatial information can lead to inferior results. The predictors using only spatial data can easily be harmed by the bias and drift from neighbourhood sensing data. Moreover, in some outdoor environment, the spatial information about the sensors could be erroneous or missing in themselves. Therefore, applying both temporal and spatial information on the sensory missing data problem seems inevitable. 
Below we will introduce several such methods. The first major differences between those models and ours is that them rely more on the spatial correlation than temporal correlation, while our model does not require specific spatial infromation as it tries to learn sensor correlations from data.   

An spatial and temporal imputation (STI) altorithm is proposed in \cite{li2008spatial}. 
For each missing record, STI first checks if some nodes are within the sensing neighborhood, and utilize 
the average of the neighbours to impute the missing value. If no sensors are in the neighborhood, the last seen value of the missing sensor is used for imputation. The method favors spatial info than temporal info. It has the advantage of simple, fast, but requires some smart indexing technique to efficiently reterive neighborhood information. 
However, problems can arise if the last seen value is far away from the current time stamp, and when most of the nodes in the sensing region has high missing rate. 
%Furthermore, the usage of circular region is problematic as most of the landforms might not be uniformly distributed. Finally, determining the range of the radius is tricky, as a larger radius can cause noise while a smaller one cannot highlight the effectiveness of this model(i.e. the results will be similar to linear interpolation).
%The radius of the region is sometimes too large for sensors whose correlations with the target sensor are low in the sensing range.  

Another hybrid algorithm replying more on spatial than temporal is the Data Estimation using Statistical Model (DESM)\cite{li2008data}.  
The prediction function of sensor $i$ is:
%\begin{equation}
$\hat{y_{it}} = (1-\alpha)\hat{y_{i(t-1)}} + (\alpha)\hat{z}$
%\end{equation}

where $\hat{y_{i(t-1)}}$ and $\hat{y_{i(t-1)}}$ represent the signal value in the current and the previous time stamp, and $\alpha$ is the Pearson correlation coefficient between the sensor $i$ and the nearest sensor $j$.
The above equation assumes that the sensor $X_i$ and $X_j$ have the similar data fluctuation trend. 
%$\alpha$ is expected to have more impact on the prediction if $X_i$ is more correlated with $X_j$. 
$\hat{y_{it}}$ is the last estimated value, which measures the influence of the historical sensing data on the current value at node i. 
$\hat{z}$ is the estimation of sensor $j$ based on the observation from sensor $i$. 
$\hat{z} = X_{j(t-1)}(1+\frac{X_{i(t)}-X_{i(t-1)}}{X_{i(t-1)}})$
It measures the influence of the data sensed by the active node i on the data of the nearest node j.  
%\begin{equation}
%\label{z-hat}
%\end{equation}
In DESM, because $\alpha$ is usually high around the target sensor. Therefore, the temporal feature multiplied by $1-\alpha$ holds a very small portion of contribution to the prediction process. It often contradicts the nature that the temporal correlation itself is usually higher than any spatial correlation with other sensors. The performance may becomes bad if the nearest sensor is not the most correlated one, i.e. blocked by a wall.
% \subsubsection{Applying K-nearest neighbour Estimation}

K-nearest neighbours algorithm(KNN) is an intuitive yet effective spatial correlation based imputation method. It directly uses the weighted average of the neighbor signals to fill the missing data, and has been adopted to successfully estimate the missing values of DNA micro-arrays\cite{Troyanskaya:DNAKNN}.  
%While in WSNs, the sensor data of different nodes is more likely to have some functional relations other than just using sensor values.
Pan\cite{pan2010k} propose the Applying K-nearest neighbour Estimation algorithm(AKE) to exploit the spatial correlation in the missing sensory data problem. They argue that the past research on sensor imputation(STI) estimates the missing problem based on more temporal correlation than spatial correlation(STI). In their observation, the sensor data in WSN often changes rapidly and sharply especially in outside environment. The AKE adopts the linear regression model,
%\begin{equation}
$\hat{y}_{it} =\hat{\alpha} +\hat{\beta}\cdot y_{jt}$
%\label{ake_j}
%\end{equation}
 where $it$ means the time t of sensor i,  and j is its neighbour nodes.  
They claim that using a linear combination of the estimation from neighbour sensors can lower the random error caused by only using a single sensor. 
For given node $i$ whose data is missing at time $t$, and the m nearest neighbour sensors who have values at time $t$[$y_{1t}, y_{2t},\dots, y_{mt}$], the missing value is predicted by :   
 \begin{equation}
 \hat{y}_{it} =\sum_{j=1}^m w_j \cdot \hat{y}_{it}^{(j)}
 \label{ake_impute}
 \end{equation}
% First they build a matrix 
%\[ \left( \begin{array}{cccc}
%
%\end{array}
%\right)
%\]
In eq\ref{ake_impute}, the estimated $\hat{y}_{it}^{j}$ is computed by node $j$ using their linear regression coefficients($\alpha$, $\beta$). In designing the weighting function, the AKE gives higher correlation pairs a larger weight. They adopts the r-square statistics to rank their correlations due to when sensors have the higher $R^2$, they have the better regression equation fit. 
%Based on the (eq\ref{ake_j}), the $R^2$ is computed by:
%\begin{equation}
%R_(j)^2 = \sum_{n=1}^{h} \frac{(\hat{y}_{in}^(j)-\bar{y}_i)^2}{(y_{in}-\bar{y}_i)^2}
%\end{equation}
%Previous study [2] has showed that for high sampling rates sensing, the temporal correlation is higher than the spatial correlation from nearby sensors. 
%Therefore, if the consecutive missing frames are within the threshold learned by AKE , the linear interpolation is used to predict the value. 
However, in some environment, the distance used to decide the m nearest neighbours is not feasible. 
Moreover, in certain scenarios where two far away sensors have higher correlation, i.e. two sensors place under the same air-conditioning tuyere, the AKE will ignore the far away sensor.
Finally, the preference on spatial over temporal is also being challenged when the sampling rates is very high, i.e. the body sensors, using temporal linear interpolation has better prediction.   

%\subsubsection{Imputation Method-ImM}
% In \cite{Lim:robust}, an imputation method(ImM) consists of two temporal and one spatial predictors was introduced.  The algorithm argues that it is not robust to set the preference for its spatial predictor over temporal predictor, or vice versa. They assume that the predictor which yields the most accurate results in the training data set is used to predict the missing packets. 
%The first temporal predictor(LRS) uses the last transmitted packet as the predicted value eq\ref{lrs}.
%\begin{equation}
%\hat{y}_{t} =y_{t-1} \label{lrs}
%\end{equation} 
% The second temporal predictor uses a linear predictor(LP). Given $o$, the training samples of specific sensor $p$, and $O$ who is the training data of sensors other than $p$. The coefficients of the linear predictor is decided by autocorrelation method of autoregressive modelling. The predicted value by eq\ref{lp}:
%\begin{equation}
%\hat{y}_{t}^p = a(2)X_o(t-1) - a(3)X_o(t-2)- \dots -a(p+1)X_o(t-p) \label{lp}
%\end{equation} 
%where $p$ is the sensor node ID.  
%The spatial predictor uses the linear predictive model, given the nodes who are successively transmitted to the sink node, $p_1,p_2,\dots,p_L$, the predicted value is(eq\ref{ImM_Spatial}): 
%\begin{equation}
%\hat{y}_{t}^p = \sum_{l=1}^L a_lp_l
%\label{ImM_Spatial}
%\end{equation}
% where the weights is determined by weighted coefficients(WC) using least square method$[a_1,a_2,\dots,a_L]^T = (O^TO)^{-1}O^To$.
%However, the performance of the ImM may degrades if the changes in the continuous data stream are sharp in the high sampling rate data set. 
%For example, the wind sensors in the windy climate area may affect the method applied on the target sensor.  

\subsection{Matrix Decomposition Models}
%\subsubsection{Prediction based on Empirical Orthogonal Functions }
The Matrix decomposition model is the one that is closest to our solution. Here we introduce the Empirical Orthogonal Functions(EOF), which has been applied to oceanographic application to solve the problem of missing or unreliable satellite data\cite{beckers2003eof}. 
They propose a Singular Value Decomposition(SVD)-based technique for the reconstruction of missing data. In EOF, they have to first fill in the missing values (e.g. using all zero or mean values) and then perform SVD to decompose the matrix. Then they use the first k spatial-EOF (i.e. the top-k singular values in SVD) to reconstruct the matrix (i.e. perform dimension reduction) for imputation, where k is determined through cross validation over data.
Researchers have pointed out several drawbacks in the SVD-based methods when applying to impute large-scale sensors network. First, given large data, performing SVD is extremely computational demanding as the inverse of a dense matrix is required. Second, it requires to fill all the missing values before the decomposition can be performed, and imperfect initial imputation (i.e. reseachers argue that filling all zeros is questionable) can significantly hurt the final imputation results, in particular when the missing ratio is high \cite{koren2009matrix}\cite{ke2005robust}. 
  
%\subsubsection{Matrix Factorization on Structure of Motion} 
%tructure from motion(SFM)\cite{tomasi1992shape} and motion estimation are among the most important application in computer vision.   A number of studies have investigated different approaches to the MF-based structure from motion problem. \cite{buchanan2005damped} performs the L2-norm minimization using the standard Newton algorithm with a damping factor. In \cite{okatani2011efficient}, they propose a new method incorporates the damping factor into the Wiberg method to solve the L2-norm problem. A L1-norm factorization using Wiberg method is proposed in\cite{eriksson2010efficient}. 
%They turn the objective function into a non-linear least square problem and adopt Gauss-Newton method to solve the problem. However, the size and ranks of the video data is different from the long-term and densely deployed sensor data. The rank of linear subspace in SFM has rank of three after it is subtracted by its column-wise mean. It is also noted that the video camera in essential is an univariate sensor device which is different from the nature of heterogeneity of sensor networks. 